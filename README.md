# ğŸ¤– Chatbot GenAI - Caso de Estudio experto en normas e infracciones de transito en colombia.

Este proyecto presenta la solucion al desafÃ­o para estudiantes de cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps** basado en workshop [GenAIOps_Pycon2025](https://github.com/darkanita/GenAIOps_Pycon2025/blob/587125e05f1c99b36f4da80641b42826521c96b5/README.md) propuesto por la profesora [darkanita](https://github.com/darkanita).

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre el codigo nacional de transito e infracciones, usando como base de conocimeinto documentos PDF con la ley 769 del codigo nacional de transito, ley 1548 de 2012 y un manual de infracciones.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ improved_main_interface.py â† interfaz mejorada desafio propuesto
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_transito.txt
â”‚       â””â”€â”€ v2_resumido_directo.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_dataset.json         â† dataset de evaluaciÃ³n
â”‚   â””â”€â”€ eval_dataset.csv
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

## ğŸ“ DesafÃ­o para estudiantes

ğŸ§© Parte 1: PersonalizaciÃ³n

1. Elige un nuevo dominio: legal,normatividad de transito.âœ…

2. Reemplaza los documentos PDF: UbÃ­calos en data/pdfs/.âœ…

3. Modifica o crea tus prompts: Edita los archivos en app/prompts/.âœ…

4. Crea un conjunto de pruebas
En [tests/eval_dataset.json](tests/eval_dataset.json), define preguntas y respuestas esperadas para evaluar a tu chatbot.âœ…

âœ… Parte 2: EvaluaciÃ³n AutomÃ¡tica

1. Ejecuta run_eval.py para probar tu sistema actual.(Cambiar varaible de entorno con el nombre de la version del prompt seleccionado)

```bash
python app/run_eval.py
```
Actualmente, la evaluaciÃ³n estÃ¡ basada en QAEvalChain de LangChain, que devuelve una mÃ©trica binaria: correcto / incorrecto.

validar resultados en mlflow.

```bash
mlflow ui --port 5000
```
![metricas run_eval](image.png)
![run_eval question 1](image-1.png)

ğŸ”§ Parte 3: Â¡Tu reto! (ğŸ‘¨â€ğŸ”¬ nivel investigador)

1. Mejora el sistema de evaluaciÃ³n:

    * Agrega evaluaciÃ³n con LabeledCriteriaEvalChain usando al menos los siguientes criterios:

        * "correctness" â€“ Â¿Es correcta la respuesta?
        * "relevance" â€“ Â¿Es relevante respecto a la pregunta?
        * "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
        * "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
        * "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?

    * Cada criterio debe registrar:

        * Una mÃ©trica en MLflow (score)

    * Y opcionalmente, un razonamiento como artefacto (reasoning)

    ğŸ“š Revisa la [documentaciÃ³n de LabeledCriteriaEvalChain](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html) para implementarlo.

    ![metricas y parametros con LabeledCriteriaEvalChain](image-2.png)
    ![artefactos con LabeledCriteriaEvalChain](image-3.png)

ğŸ“Š Parte 4: Mejora el dashboard

1. Extiende dashboard.py o main_interface.py para visualizar:

    * Las mÃ©tricas por criterio (correctness_score, toxicity_score, etc.).
    * Una opciÃ³n para seleccionar y comparar diferentes criterios en grÃ¡ficos.
    * (Opcional) Razonamientos del modelo como texto.

VersiÃ³n mejorada con mÃ©tricas:
```bash
streamlit run app/improved_main_interface.py
```
![mÃ©tricas experimento por pregunta](image-6.png)

![metricas consolidadas por tamaÃ±o chunk y prompt](image-11.png)

![comparar metricas en grafico](image-4.png)

![Razonamientos del modelo como texto](image-5.png)
---

ğŸ§ª Parte 5: Presenta y reflexiona
1. Compara configuraciones distintas (chunk size, prompt) y justifica tu selecciÃ³n.
    * Â¿CuÃ¡l configuraciÃ³n genera mejores respuestas? -> sin importar el tamaÃ±o del chunk,
    el prompt version v1_asistente_transito, fue el que dio mejores resultados. ver imagen v1_asistente_transito.
    * Â¿En quÃ© fallan los modelos? -> En las respuestas con aluccinaciones(4/10) y el numero de respuestas correctas (6/10).

    ![Fallas modelos](image-12.png)

    * Usa evidencias desde MLflow y capturas del dashboard.

cambiar valore de variables de entorno para cuatro escenarios de experimentaciÃ³n:

configuracion 1:

```bash
PROMPT_VERSION=v1_asistente_transito
CHUNK_SIZE=512
CHUNK_OVERLAP=50
EVAL_METHOD='criteria_eval'
```
![v1_asistente_transito](image-7.png)

configuracion 2:

```bash
PROMPT_VERSION=v2_resumido_directo
CHUNK_SIZE=512
CHUNK_OVERLAP=50
EVAL_METHOD='criteria_eval'
```
![v2_resumido_directo](image-8.png)

configuracion 3:

```bash
PROMPT_VERSION=v1_asistente_transito
CHUNK_SIZE=1024
CHUNK_OVERLAP=100
EVAL_METHOD='criteria_eval'
```
![v1_asistente_transito](image-10.png)

configuracion 4:

```bash
PROMPT_VERSION=v2_resumido_directo
CHUNK_SIZE=1024
CHUNK_OVERLAP=100
EVAL_METHOD='criteria_eval'
```
![v2_resumido_directo](image-9.png)

```bash
python app/run_eval.py
```

ğŸš€ Bonus

- Â¿Te animas a crear un nuevo criterio como "claridad" o "creatividad"? Puedes definirlo tÃº mismo y usarlo con LabeledCriteriaEvalChain.

se creo un nuevo criterio de alucinaciones hallucination_score

---

## ğŸ§± Recuerda

###  ğŸ§± PreparaciÃ³n del entorno

```bash
git clone https://github.com/AndresR2909/GenAIOps_Pycon2025.git chatbot-genaiops
git checkout expert_in_Colombian_traffic_regulations
cd chatbot-genaiops
conda create -n chatbot-genaiops python=3.10 -y
conda activate chatbot-genaiops
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore();"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**


â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_asistente_transito")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

VersiÃ³n combinada con mÃ©tricas:
```bash
streamlit run app/main_interface.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Eval (QAEvalChain)`
- Registra resultados en **MLflow**

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de precisiÃ³n por configuraciÃ³n (`prompt + chunk_size`)
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions

- CI de evaluaciÃ³n: `.github/workflows/eval.yml`
- Test unitarios: `.github/workflows/test.yml`

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas
- ğŸ§ª Trazar todo en MLflow
- ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³nâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica
- **GitHub Actions** â€“ CI/CD
- **DevContainer** â€“ Desarrollo portable

---
